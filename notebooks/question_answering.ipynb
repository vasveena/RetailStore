{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b46b5f",
   "metadata": {},
   "source": [
    "<h1> Answer questions by querying the database using Amazon Bedrock </h1>\n",
    "<br>\n",
    "\n",
    "<p> This notebook highlights an example of question answering with the help of SQL generation. \n",
    "    \n",
    "<p> Before starting, please make sure this notebook is using <b>conda_python3</b> kernel from the top right! </p>\n",
    "\n",
    "<p> In this notebook, let's learn how to use Amazon Bedrock to answer user questions in natural language </p>\n",
    "\n",
    "<p> To run this notebook, go to Cell -> Run All. Inspect the output of each cell block. </p>\n",
    "\n",
    "<b> Please read the following instructions carefully! </b>\n",
    "    <ul>\n",
    "    <li>We highly recommend you to run all cells and inspect output rather than running the cells individually to save time as well as avoid any issues.  \n",
    "    <li>This notebook is for your understanding. Running this notebook is NOT required for proceeding with the next steps of your workshop.\n",
    "    <li>In case your notebook does not run as expected or if you run into any errors, please proceed with the next steps provided in the Workshop instructions. \n",
    "    <li>If you choose to run the notebooks, please read the comments in the markdown and inspect the output of each cell.\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dcf4cd",
   "metadata": {},
   "source": [
    "<h3> Install required dependencies </h3>\n",
    "<p> <b>Note:</b> If you notice any ERRORs from the following cell, ignore them and proceed with the next cells.</p><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823631f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --no-build-isolation --upgrade \\\n",
    "    \"boto3==1.28.63\" \\\n",
    "    \"awscli==1.29.63\" \\\n",
    "    \"botocore==1.31.63\" \\\n",
    "    \"langchain==0.0.309\" \\\n",
    "    \"psycopg2-binary==2.9.9\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97e23c",
   "metadata": {},
   "source": [
    "<h3> Import required packages </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e4ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import botocore\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "import psycopg2\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5637d89",
   "metadata": {},
   "source": [
    "<h3> Initialize Bedrock client </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11248a45",
   "metadata": {},
   "source": [
    "#### Get bucket name that contains all the artifacts we need for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "s3_bucket = None\n",
    "response = s3.list_buckets()\n",
    "for bucket in response['Buckets']:\n",
    "    if 'reinvent-retails-bucket' in bucket['Name']: \n",
    "        s3_bucket = bucket['Name']\n",
    "\n",
    "print(s3_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cea604",
   "metadata": {},
   "source": [
    "#### Read schema file that contains schema of all the metadata stored in Amazon RDS for our Django application\n",
    "\n",
    "Typically, all the data from web applications are stored in a database. In our case, all the data of our re:Invent retails web application (like products, orders, ratings etc.) are stored in RDS PostgresDB. You can easily obtain schema of the table with the [pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html) tool. We have already taken a dump of the table schemas from RDS tables and uploaded it to S3. Let's read this schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128df429",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_response = s3.get_object(Bucket=s3_bucket, Key=\"data/schema-mysql.sql\")\n",
    "schema = s3_response['Body'].read().decode(\"utf-8\")\n",
    "\n",
    "# Printing schemas of all the tables used in our web application\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44cbe3c",
   "metadata": {},
   "source": [
    "#### Create prompt template\n",
    "\n",
    "<p>This prompt template will generate an SQL query based on the schema passed above. We are passing PostgresQL documentation to help with the SQL generation. Read the prompt instructions carefully. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efefb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "            Human: Create an Postgres SQL query for a retail website to answer the question keeping the following rules in mind: \n",
    "            \n",
    "            1. Database is implemented in Postgres SQL.\n",
    "            2. Postgres syntax details can be found here: https://www.postgresql.org/files/documentation/pdf/15/postgresql-15-US.pdf\n",
    "            3. Enclose the query in <query></query>. \n",
    "            4. Use \"like\" and upper() for string comparison on both left hand side and right hand side of the expression. For example, if the query contains \"jackets\", use \"where upper(product_name) like upper('%jacket%')\". \n",
    "            5. If the question is generic, like \"where is mount everest\" or \"who went to the moon first\", then do not generate any query in <query></query> and do not answer the question in any form. Instead, mention that the answer is not found in context.\n",
    "            6. If the question is not related to the schema, then do not generate any query in <query></query> and do not answer the question in any form. Instead, mention that the answer is not found in context.  \n",
    "		 7. If the question is asked in a language other than English, convert the question into English before constructing the SQL query. The string and varchar fields stored in the database are always in English.  \n",
    "\n",
    "            <schema>\n",
    "                {schema}\n",
    "            </schema>\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Assistant:\n",
    "            \n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2993d23",
   "metadata": {},
   "source": [
    "#### Now, lets ask a question in natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7568ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many products do you have in your store?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25bfe3d",
   "metadata": {},
   "source": [
    "#### Pass this question to prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88453155",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_vars = PromptTemplate(template=prompt_template, input_variables=[\"question\",\"schema\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2225767",
   "metadata": {},
   "source": [
    "#### Now lets call the LLM to generate SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Claude Anthropic LLM\n",
    "llm = Bedrock(model_id=\"anthropic.claude-instant-v1\", client=boto3_bedrock)\n",
    "\n",
    "# Pass question and postgres schema of the web application\n",
    "prompt = prompt_vars.format(question=question, schema=schema)\n",
    "\n",
    "# Print response\n",
    "llm_response = llm(prompt)\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e6517",
   "metadata": {},
   "source": [
    "#### As you can see from the above llm_response, the query is embedded inside \\<query\\>\\<\\/query\\> tags. Let's create a function retrieve the query inside the \\<query\\> tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c518b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_strings_recursive(test_str, tag):\n",
    "    # finding the index of the first occurrence of the opening tag\n",
    "    start_idx = test_str.find(\"<\" + tag + \">\")\n",
    " \n",
    "    # base case\n",
    "    if start_idx == -1:\n",
    "        return []\n",
    " \n",
    "    # extracting the string between the opening and closing tags\n",
    "    end_idx = test_str.find(\"</\" + tag + \">\", start_idx)\n",
    "    res = [test_str[start_idx+len(tag)+2:end_idx]]\n",
    " \n",
    "    # recursive call to extract strings after the current tag\n",
    "    res += extract_strings_recursive(test_str[end_idx+len(tag)+3:], tag)\n",
    " \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a46879",
   "metadata": {},
   "source": [
    "#### Get the query using the function extract_strings_recursive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a003149",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\n",
    "\n",
    "if \"<query>\".upper() not in llm_response.upper():\n",
    "    is_query_generated  = False\n",
    "else:\n",
    "    is_query_generated  = True\n",
    "    query = extract_strings_recursive(llm_response, \"query\")[0]\n",
    "    print(\"Query generated by LLM: \\n\" +query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ccd331",
   "metadata": {},
   "source": [
    "#### Now, let's connect to the RDS database and run this LLM-generated query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize secrets manager\n",
    "secrets = boto3.client('secretsmanager')\n",
    "\n",
    "sm_response = secrets.get_secret_value(SecretId='postgresdb-secrets')\n",
    "\n",
    "database_secrets = json.loads(sm_response['SecretString'])\n",
    "\n",
    "# Get secrets from the secrets manager\n",
    "dbhost = database_secrets['host']\n",
    "dbport = database_secrets['port']\n",
    "dbuser = database_secrets['username']\n",
    "dbpass = database_secrets['password']\n",
    "dbname = database_secrets['name']\n",
    "\n",
    "# Connect to PostgreSQL database\n",
    "dbconn = psycopg2.connect(host=dbhost, user=dbuser, password=dbpass, port=dbport, database=dbname, connect_timeout=10)\n",
    "dbconn.set_session(autocommit=True)\n",
    "cursor = dbconn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297862cb",
   "metadata": {},
   "source": [
    "#### Execute the LLM-generated query in Amazon RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61273a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the extracted query\n",
    "cursor.execute(query)\n",
    "\n",
    "query_result = cursor.fetchall()\n",
    "dbconn.close()\n",
    "\n",
    "resultset = ''\n",
    "if len(query_result) > 0:\n",
    "    for x in query_result:\n",
    "        resultset = resultset + ''.join(str(x)) + \"\\n\"\n",
    "\n",
    "print(\"Query result: \\n\" +resultset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b5251",
   "metadata": {},
   "source": [
    "#### Now, lets create another prompt template to interpret the query results. \n",
    "\n",
    "This prompt template defines rules while describing query result. This is the final result that will be seen by the user as an answer to their question. Idea is to derive natural language answer for a natural language question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "\n",
    "        Human: This is a Q&A application. We need to answer questions asked by the customer at an e-commerce store. \n",
    "        The question asked by the customer is {question}\n",
    "        \n",
    "        We ran an SQL query in our database to get the following result. \n",
    "\n",
    "        <resultset>\n",
    "            {resultset}\n",
    "        </resultset>\n",
    "\n",
    "        Summarize the above result and answer the question asked by the customer keeping the following rules in mind: \n",
    "        \n",
    "        1. Don't make up answers if <resultset></resultset> is empty or none. Instead, answer that the item is not available based on the question.\n",
    "        2. Mask the PIIs phone, email and address if found the answer with \"<PII masked>\"\n",
    "        3. Don't say \"based on the output\" or \"based on the query\" or \"based on the question\" or something similar.  \n",
    "        4. Keep the answer concise. \n",
    "        5. Don't give an impression to the customer that a query was run. Instead, answer naturally. \n",
    "\n",
    "        Assistant:\n",
    "\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008cfaf",
   "metadata": {},
   "source": [
    "#### Create prompt and invoke LLM to interpret results retrieved from the RDS database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c07b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_vars = PromptTemplate(template=prompt_template, input_variables=[\"question\",\"resultset\"])\n",
    "\n",
    "# Pass question and result set to prompt template\n",
    "prompt = prompt_vars.format(question=question, resultset=resultset)\n",
    "\n",
    "# Call LLM to interpret query result\n",
    "describe_query_result = llm(prompt)\n",
    "\n",
    "print(\"Question asked by the user: \\n\")\n",
    "print(question + \"\\n\\n\")\n",
    "print(\"Answer interpreted by LLM: \\n\")\n",
    "print_ww(describe_query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d99689",
   "metadata": {},
   "source": [
    "<h3> You've successfully created a Q&A application using SQL generation with Amazon Bedrock!</h3>\n",
    "\n",
    "<p> Please stop the notebook kernel before proceeding. </p>\n",
    "\n",
    "<h4> Now, let's learn how to integrate Amazon Bedrock into your web application to do the same. Please go back to Workshop Studio and follow the instructions to replicate this code into your Cloud9 environment. </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9ad10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
