{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd4d0db",
   "metadata": {},
   "source": [
    "<h1> Generate Product Description Using Amazon Bedrock </h1>\n",
    "<br>\n",
    "\n",
    "<p> Before starting, please make sure this notebook is using <b>conda_python3</b> kernel from the top right! </p>\n",
    "\n",
    "<p> In this notebook, let's learn how to generate product description using Amazon Bedrock and <a href=\"https://python.langchain.com/docs/get_started/introduction\" target=\"_blank\">Langchain</a>. </p>\n",
    "\n",
    "<p> To learn more about Amazon Bedrock integration with LangChain, please make a note of this <a href=\"https://python.langchain.com/docs/integrations/llms/bedrock\" target=\"_blank\">documentation</a> to take a look at <b>after</b> the completion of this workshop. </p>\n",
    "\n",
    "<p> To run this notebook, go to Cell -> Run All. Inspect the output of each cell block. </p>\n",
    "\n",
    "<b> Please read the following instructions carefully! </b>\n",
    "    <ul>\n",
    "    <li>We highly recommend you to run all cells and inspect output rather than running the cells individually to save time as well as avoid any issues.  \n",
    "    <li>This notebook is for your understanding. Running this notebook is NOT required for proceeding with the next steps of your workshop.\n",
    "    <li>In case your notebook does not run as expected or if you run into any errors, please proceed with the next steps provided in the Workshop instructions. \n",
    "    <li>If you choose to run the notebooks, please read the comments in the markdown and inspect the output of each cell.\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18477242",
   "metadata": {},
   "source": [
    "<h3> Install required dependencies </h3>\n",
    "<p> <b>Note:</b> If you notice any ERRORs from the following cell, ignore them and proceed with the next cells. This command may take 1-2 mins to run.</p><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527cbd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --no-build-isolation --force-reinstall \\\n",
    "    \"boto3==1.28.63\" \\\n",
    "    \"awscli==1.29.63\" \\\n",
    "    \"botocore==1.31.63\" \\\n",
    "    \"langchain==0.0.309\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c8689",
   "metadata": {},
   "source": [
    "<h3> Import required packages </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c496d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import botocore\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b503ee9",
   "metadata": {},
   "source": [
    "<h3> Initialize Bedrock client </h3><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb975aa",
   "metadata": {},
   "outputs": [],
   "source": [
    " boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b524f8",
   "metadata": {},
   "source": [
    "<h3>Set inference parameters for LLM</h3>\n",
    "<p> Setting these values are optional. The parameters change based on the LLM used. In this case, we are going to use Claude Anthropic LLM. For more details, refer to this <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html\" target=\"_blank\">documentation</a>. </p><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8026e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_modifier = {}\n",
    "inference_modifier['max_tokens_to_sample'] = 200\n",
    "inference_modifier['temperature'] = 0.5\n",
    "inference_modifier['top_k'] = 250\n",
    "inference_modifier['top_p'] = 1\n",
    "inference_modifier['stop_sequences'] = [\"\\n\\nHuman\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcea204",
   "metadata": {},
   "source": [
    "<h3>Provide details about a product</h3>\n",
    "<p> This data will be used to construct our prompt which will be passed to the LLM to generate product description. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847198c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_brand = \"Legendaire\"\n",
    "product_category = \"Shirt\"\n",
    "product_name = \"Legandaire Shirt\"\n",
    "\n",
    "# Brief data about the product\n",
    "product_details = \"\"\"\n",
    "        - collared white shirt \n",
    "        - 80% cotton 20% polyester\n",
    "        - semi-casual\n",
    "        - great for office or golfing\n",
    "        - comfortable breathable material\n",
    "        - flex fit\n",
    "    \"\"\"\n",
    "\n",
    "# Colors available for the product \n",
    "product_colors = [\"White\", \"Black\", \"Blue\"]\n",
    "\n",
    "# Length of the desired product description (generated from LLM)\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc02508",
   "metadata": {},
   "source": [
    "<h3> Initialize LLM </h3>\n",
    "<p> Using Langchain, initialize an LLM instance for text generation. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af17ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing Anthropic Claude as model ID\n",
    "\n",
    "textgen_llm = Bedrock(\n",
    "            model_id=\"anthropic.claude-instant-v1\",\n",
    "            client=boto3_bedrock,\n",
    "            model_kwargs=inference_modifier,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb8c01",
   "metadata": {},
   "source": [
    "<p> Create a prompt template with variables like product details, category, brand, available colors etc. Read the template parameter carefully to understand the prompting instructions. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3fb499",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "            input_variables=[\"brand\", \"colors\", \"category\", \"length\", \"name\",\"details\"], \n",
    "            template=\"\"\"\n",
    "            \n",
    "                Human: Create a catchy product description for a {category} from the brand {brand}. \n",
    "                Product name is {name}. \n",
    "                The number of words should be less than {length}. \n",
    "\n",
    "                Following are the product details:  \n",
    "\n",
    "                <product_details>\n",
    "                {details}\n",
    "                </product_details>\n",
    "\n",
    "                Briefly mention about all the available colors of the product.\n",
    "\n",
    "                Example: Available colors are Blue, Purple and Orange. \n",
    "\n",
    "                If the <available_colors> is empty, don't mention anything about the color of the product.\n",
    "\n",
    "                <available_colors>\n",
    "                {colors}\n",
    "                </available_colors>\n",
    "\n",
    "                Assistant:\n",
    "                \n",
    "                \"\"\"\n",
    "    \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c7503",
   "metadata": {},
   "source": [
    "<p> Let's construct the prompt by passing all the variables to the prompt template. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Pass in form values to the prompt template\n",
    "\n",
    "prompt = prompt_template.format(brand=product_brand, \n",
    "                                colors=product_colors,\n",
    "                                category=product_category,\n",
    "                                length=max_length,\n",
    "                                name=product_name,\n",
    "                                details=product_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f49a8",
   "metadata": {},
   "source": [
    "<h3> Call Bedrock with the constructed prompt </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd5902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = textgen_llm(prompt)\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e41af",
   "metadata": {},
   "source": [
    "<h4> Get the second paragraph i.e, only the product description </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b880850",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_response = response[response.index('\\n')+1:]\n",
    "print_ww(generated_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fd554f",
   "metadata": {},
   "source": [
    "<h4> We can also try to generate the product descripton using another LLM. Let's use Amazon Titan LLM from Bedrock </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87256f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Amazon Titan LLM from Bedrock\n",
    "textgen_llm = Bedrock(\n",
    "                model_id=\"amazon.titan-tg1-large\",\n",
    "                client=boto3_bedrock)\n",
    "\n",
    "# Call the LLM with same prompt\n",
    "response = textgen_llm(prompt)\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b12bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04ee40fb",
   "metadata": {},
   "source": [
    "<h3> You've successfully generated description for a product with Anthropic Claude LLM!</h3>\n",
    "\n",
    "<p> Please stop the notebook kernel before proceeding. </p>\n",
    "\n",
    "<h4> Now, let's learn how to integrate Amazon Bedrock and Langchain into your web application to do the same. Please go back to Workshop Studio and follow the instructions to replicate this code into your Cloud9 environment. </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c65ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
